{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
      "colab": {
        "name": "3_Least_squares.ipynb",
        "provenance": [],
        "collapsed_sections": [],
        "include_colab_link": true
      },
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      }
    },
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "view-in-github",
          "colab_type": "text"
        },
        "source": [
          "<a href=\"https://colab.research.google.com/gist/jonghank/e0032bfb083e451628e989a070b0c375/least_squares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "GY_tlKGhFh6V"
        },
        "source": [
          "# Least squares, least norm, and constrained least squares problems\n",
          "\n",
          "$$\n",
          "\\newcommand{\\eg}{{\\it e.g.}}\n",
          "\\newcommand{\\ie}{{\\it i.e.}}\n",
          "\\newcommand{\\argmin}{\\operatornamewithlimits{argmin}}\n",
          "\\newcommand{\\mc}{\\mathcal}\n",
          "\\newcommand{\\mb}{\\mathbb}\n",
          "\\newcommand{\\mf}{\\mathbf}\n",
          "\\newcommand{\\minimize}{{\\text{minimize}}}\n",
          "\\newcommand{\\diag}{{\\text{diag}}}\n",
          "\\newcommand{\\cond}{{\\text{cond}}}\n",
          "\\newcommand{\\rank}{{\\text{rank }}}\n",
          "\\newcommand{\\range}{{\\mathcal{R}}}\n",
          "\\newcommand{\\null}{{\\mathcal{N}}}\n",
          "\\newcommand{\\tr}{{\\text{trace}}}\n",
          "\\newcommand{\\dom}{{\\text{dom}}}\n",
          "\\newcommand{\\dist}{{\\text{dist}}}\n",
          "\\newcommand{\\R}{\\mathbf{R}}\n",
          "\\newcommand{\\SM}{\\mathbf{S}}\n",
          "\\newcommand{\\ball}{\\mathcal{B}}\n",
          "\\newcommand{\\bmat}[1]{\\begin{bmatrix}#1\\end{bmatrix}}\n",
          "$$"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "dcn4VDPcFh6Y"
        },
        "source": [
          "__<div style=\"text-align: right\"> ASE7030: Convex Optimization, Inha University. </div>__\n",
          "_<div style=\"text-align: right\"> Jong-Han Kim (jonghank@inha.ac.kr) </div>_\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "88SCE4mGehwg"
        },
        "source": [
          "<br>\n",
          "\n",
          "## Least squares\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "_IQRBv-7egb0"
        },
        "source": [
          "<br>\n",
          "\n",
          "### Least squares problems\n",
          "\n",
          "$$\n",
          "\\begin{aligned}\n",
          "  \\underset{x}{\\minimize} \\quad& \\| Ax-b \\|_2^2\n",
          "\\end{aligned}\n",
          "$$\n",
          "\n",
          "with $A$ being tall and full rank (columns of $A$ linearly independent). The optimal solution is given by\n",
          "\n",
          "\\begin{align*}\n",
          "x^* &= A^\\dagger b \\\\\n",
          "&= (A^TA)^{-1}A^Tb\n",
          "\\end{align*}\n",
          "\n",
          "Questions:\n",
          "- Why $A^\\dagger b$?\n",
          "- What if $A$ is rank deficient (columns of $A$ linearly dependent)?\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "JTUYx2dWet8x"
        },
        "source": [
          "<br>\n",
          "\n",
          "### Regularized least squares problems\n",
          "\n",
          "$$\n",
          "\\begin{aligned}\n",
          "  \\underset{x}{\\minimize} \\quad& \\| Ax-b \\|_2^2 + \\lambda \\|x\\|_2^2\n",
          "\\end{aligned}\n",
          "$$\n",
          "\n",
          "with some _hyper-parameter_ $\\lambda>0$. This is equivalent to\n",
          "\n",
          "$$\n",
          "\\begin{aligned}\n",
          "  \\underset{x}{\\minimize} \\quad& \\left\\| \\bmat{A \\\\ \\sqrt{\\lambda}I}x-\\bmat{b\\\\0} \\right\\|_2^2\n",
          "\\end{aligned}\n",
          "$$\n",
          "\n",
          "Hence the optimal solution is given by: \n",
          "$$\n",
          "x^* = \\left( A^TA+\\lambda I\\right)^{-1}A^Tb\n",
          "$$\n",
          "\n",
          "Note that the above always holds for any $A$ (full rank assumption not required).\n",
          "\n",
          "Questions:\n",
          "- What happens if $\\lambda\\rightarrow \\infty$?\n",
          "- What happens if $\\lambda\\rightarrow 0$ when $A$ is tall full rank?\n",
          "- What happens if $\\lambda\\rightarrow 0$ when $A$ is wide full rank?\n",
          "\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "4x7_rMrcg-h2"
        },
        "source": [
          "\n",
          "<br> \n",
          "\n",
          "### Least norm problems\n",
          "\n",
          "$$\n",
          "\\begin{aligned}\n",
          "  \\underset{x}{\\minimize} \\quad& \\|x \\|_2^2 \\\\\n",
          "  \\text{subject to} \\quad& Ax=b\n",
          "\\end{aligned}\n",
          "$$\n",
          "\n",
          "with $A$ being wide and full rank (rows of $A$ linearly independent). The optimal solution is given by\n",
          "\n",
          "\\begin{align*}\n",
          "  x^* &= A^\\dagger b \\\\\n",
          "  &= A^T(AA^T)^{-1}b\n",
          "\\end{align*}\n",
          "\n",
          "Questions:\n",
          "- Why $A^\\dagger b$?\n",
          "- What if $A$ is rank deficient (rows of $A$ linearly dependent)?\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "p1KVdQ8vunOC"
        },
        "source": [
          "\n",
          "\n",
          "<br>\n",
          "\n",
          "### Constrained least squares problems\n",
          "\n",
          "$$\n",
          "\\begin{aligned}\n",
          "  \\underset{x}{\\minimize} \\quad& \\| Ax-b \\|_2^2\\\\   \\text{subject to} \\quad& Cx=d\n",
          "\\end{aligned}\n",
          "$$\n",
          "\n",
          "with $A$ being full column rank and $C$ being full row rank (actually these can be relaxed).\n",
          "The optimal solution can be obtained by solving\n",
          "\n",
          "$$\n",
          "\\bmat{A^TA & C^T \\\\ C & 0 }\\bmat{x\\\\ \\nu} = \\bmat{A^Tb \\\\ d}\n",
          "$$\n",
          "\n",
          "where the first equation\n",
          "$$\n",
          "  A^TA x + C^T \\nu = A^T b\n",
          "$$\n",
          "states that the $\\null(C)$ is orthogonal to the gradient of $\\| Ax-b \\|_2^2$ at the optimum, and the second equation\n",
          "$$\n",
          "  Cx = d\n",
          "$$\n",
          "states that the optimal point is feasible.\n",
          "\n",
          "The optimal solution can also be obtained via solving:\n",
          "\n",
          "$$\n",
          "\\bmat{0 & A^T & C^T \\\\ A & -I & 0 \\\\ C & 0 & 0} \\bmat{x \\\\ y \\\\ \\nu} = \\bmat{0 \\\\ b \\\\ d}\n",
          "$$\n",
          "\n",
          "which is helpful especially when $A$ and $C$ are sparse.\n"
        ]
      }
    ]
  }